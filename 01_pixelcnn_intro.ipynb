{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33989cd6",
   "metadata": {},
   "source": [
    "## Run in Google Colab\n",
    "\n",
    "Click the button below to run this notebook in Google Colab (no setup required):\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/katjath/PixelCNN-Tutorial/blob/main/01_pixelcnn_intro.ipynb)\n",
    "\n",
    "Alternatively, run this cell to install dependencies in Colab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies in Colab (if running in Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch\", \"torchvision\"])\n",
    "    print(\"Installation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PixelCNN: Introduction and Tutorial\n",
    "\n",
    "This notebook introduces PixelCNN step by step. Goal: a **physically and mathematically\n",
    "accurate** explanation **and** a runnable reference implementation.\n",
    "\n",
    "**Learning goals**\n",
    "- Why autoregressive models provide exact likelihoods\n",
    "- How masked convolutions enforce causality in images\n",
    "- How training (negative log-likelihood) and sampling work\n",
    "- Physical context: relation to entropy and statistical mechanics\n",
    "\n",
    "**Runtime note**: The training cells are intentionally small. For better samples later,\n",
    "use more epochs or a larger model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We use PyTorch for building Neural Networks and train on MNIST (28x28 grayscale), a database with handwritten digits.\n",
    "\n",
    "**Seed for reproducibility**: The seed ensures that random number generation is deterministic. This means that if someone else runs this notebook with the same seed, they will get identical results. This is crucial for comparing results, debugging, and ensuring that your findings are reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and reproducibility\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory: Autoregressive Modeling\n",
    "\n",
    "An image is treated as a vector of pixels $x = (x_1, x_2, ..., x_N)$.\n",
    "By the chain rule, any distribution can be written as a product of conditional\n",
    "probabilities:\n",
    "\n",
    "$$p(x) = \\prod_{i=1}^{N} p(x_i \\mid x_{<i})$$\n",
    "\n",
    "For images we choose a raster ordering (e.g., row-wise). The model must therefore\n",
    "predict each pixel given all previous pixels.\n",
    "\n",
    "**Training**: Maximizing the log-likelihood is equivalent to minimizing the negative\n",
    "log-likelihood (NLL):\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{i=1}^{N} \\log p(x_i \\mid x_{<i})$$\n",
    "\n",
    "Because the likelihood is exactly computable, we can report *exact* log-likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PixelCNN Idea: Masked Convolutions\n",
    "\n",
    "A standard convolution uses neighbors in all directions. That would allow the model\n",
    "to see **future** pixels when predicting $x_i$. PixelCNN prevents this via **masked\n",
    "convolutions**:\n",
    "\n",
    "- **Mask A**: for the first layer (the current pixel itself is not allowed)\n",
    "- **Mask B**: for later layers (current pixel allowed, but no future pixels)\n",
    "\n",
    "This preserves autoregressive causality. The masks ensure that when predicting pixel $i$, \n",
    "the model can only use information from pixels $1, ..., i-1$, not from future pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mask A/B\n",
    "def make_mask(kernel_size, mask_type):\n",
    "    k = kernel_size\n",
    "    mask = np.ones((k, k), dtype=np.float32)\n",
    "    center = k // 2\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i > center or (i == center and j > center):\n",
    "                mask[i, j] = 0.0\n",
    "    if mask_type == 'A':\n",
    "        mask[center, center] = 0.0\n",
    "    return mask\n",
    "\n",
    "maskA = make_mask(5, 'A')\n",
    "maskB = make_mask(5, 'B')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax[0].imshow(maskA, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0].set_title('Mask A')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(maskB, cmap='gray', vmin=0, vmax=1)\n",
    "ax[1].set_title('Mask B')\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation: MaskedConv2d and PixelCNN\n",
    "We build a simple PixelCNN variant (no gating/ResNet) that shows the idea correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert mask_type in ('A', 'B')\n",
    "        self.mask_type = mask_type\n",
    "        self.register_buffer('mask', torch.ones_like(self.weight))\n",
    "        self._create_mask()\n",
    "\n",
    "    def _create_mask(self):\n",
    "        kH, kW = self.kernel_size\n",
    "        center_h = kH // 2\n",
    "        center_w = kW // 2\n",
    "        self.mask[:, :, center_h+1:, :] = 0\n",
    "        self.mask[:, :, center_h, center_w+1:] = 0\n",
    "        if self.mask_type == 'A':\n",
    "            self.mask[:, :, center_h, center_w] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Enforce causality via mask without in-place weight edits\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            self.weight * self.mask,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePixelCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_filters=64, n_layers=7, n_bins=16):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # First layer: Mask A\n",
    "        layers.append(MaskedConv2d('A', in_channels, n_filters, kernel_size=7, padding=3))\n",
    "        layers.append(nn.ReLU())\n",
    "        # Middle layers: Mask B\n",
    "        for _ in range(n_layers - 2):\n",
    "            layers.append(MaskedConv2d('B', n_filters, n_filters, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        # Output: n_bins logits per pixel\n",
    "        layers.append(MaskedConv2d('B', n_filters, n_bins, kernel_size=1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output: [B, n_bins, H, W]\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data: MNIST and Quantization\n",
    "PixelCNN models **discrete** values. We quantize grayscale values into `n_bins`\n",
    "classes (e.g., 16).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_bins = 16\n",
    "batch_size = 64\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a subset for faster training\n",
    "train_subset = Subset(train_ds, list(range(0, 5000)))\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def quantize(x, n_bins):\n",
    "    # x in [0,1] -> integer in [0, n_bins-1]\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    return (x * (n_bins - 1)).round().long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "We optimize the negative log-likelihood, which for discrete pixels is just\n",
    "cross-entropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda4cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.1606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m epochs = \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# small for demo\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: loss = \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m'\u001b[39m.format(epoch, loss))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader)\u001b[39m\n\u001b[32m     11\u001b[39m loss = F.cross_entropy(logits, xq.squeeze(\u001b[32m1\u001b[39m))\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m optimizer.step()\n\u001b[32m     15\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = SimplePixelCNN(in_channels=1, n_filters=64, n_layers=7, n_bins=n_bins).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        xq = quantize(x, n_bins)  # [B,1,H,W] int\n",
    "        logits = model(x)        # [B,n_bins,H,W]\n",
    "        loss = F.cross_entropy(logits, xq.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "epochs = 2  # small for demo\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_epoch(model, train_loader)\n",
    "    print('Epoch {}: loss = {:.4f}'.format(epoch, loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sampling (Generative Model)\n",
    "During sampling we build an image pixel by pixel. For each pixel we compute\n",
    "\\(p(x_i \\mid x_{<i})\\) and draw a sample from that distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples=16, shape=(1, 28, 28), n_bins=16):\n",
    "    model.eval()\n",
    "    C, H, W = shape\n",
    "    x = torch.zeros((n_samples, C, H, W), device=device)\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits[:, :, i, j], dim=1)\n",
    "            sample_ij = torch.multinomial(probs, num_samples=1)\n",
    "            # Rescale back to [0,1]\n",
    "            x[:, :, i, j] = sample_ij.float() / (n_bins - 1)\n",
    "    return x\n",
    "\n",
    "samples = sample(model, n_samples=16, shape=(1, 28, 28), n_bins=n_bins)\n",
    "samples = samples.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    ax[i//4, i%4].imshow(samples[i, 0], cmap='gray')\n",
    "    ax[i//4, i%4].axis('off')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Physical Context\n",
    "PixelCNN can be viewed as a probabilistic model on a lattice. In statistical mechanics,\n",
    "configurations \\(x\\) are often described by a Boltzmann distribution\n",
    "\\(p(x) \\propto \\exp(-E(x)/T)\\). The main difficulty there is the **partition function**\n",
    "(normalization), which is usually hard to compute.\n",
    "\n",
    "In the autoregressive approach, normalization is trivial because \\(p(x)\\) is explicitly\n",
    "constructed as a product of conditionals. This implies:\n",
    "- Exact log-likelihood (no approximation of the normalization)\n",
    "- Exact model entropy via the NLL\n",
    "\n",
    "Therefore PixelCNN is physically interesting when you need **exact** probabilities on\n",
    "lattice configurations, e.g., for classical spin models or as a comparison to\n",
    "energy-based models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Extensions\n",
    "- **Gated PixelCNN**: better modeling via gating and ResNet-style blocks\n",
    "- **Conditional PixelCNN**: conditioning on labels or other fields\n",
    "- **PixelCNN++**: logistic mixtures for continuous values\n",
    "\n",
    "If you want, we can implement one of these variants next or adapt the model to a\n",
    "physical lattice dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises (optional)\n",
    "1. Increase `n_bins` and observe sample quality.\n",
    "2. Train for more epochs and compare log-likelihood.\n",
    "3. Implement residual blocks or Gated PixelCNN.\n",
    "4. Test the model on a physical spin lattice.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
